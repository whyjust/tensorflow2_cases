{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow2教程-用keras构建自己的网络层\n",
    "### 1 构建一个简单的网络层\n",
    "我们可以通过继承tf.keras.layer.Layer,实现一个自定义的网络层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, print_function, division\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.12649278 -0.0621473   0.09325793 -0.1717659 ]\n",
      " [-0.12649278 -0.0621473   0.09325793 -0.1717659 ]\n",
      " [-0.12649278 -0.0621473   0.09325793 -0.1717659 ]], shape=(3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 定义网络层就是：设置网络权重和输入到输出的计算过程\n",
    "from turtle import shape\n",
    "\n",
    "\n",
    "class MyLayer(layers.Layer):\n",
    "    def __init__(self, input_dim=32, unit=32):\n",
    "        super(MyLayer, self).__init__()\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        # 权重变量\n",
    "        self.weight = tf.Variable(\n",
    "            initial_value=w_init(shape=(input_dim, unit), dtype=tf.float32),\n",
    "            trainable=True\n",
    "        )\n",
    "        # 偏置变量\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.bias = tf.Variable(\n",
    "            initial_value=b_init(shape=(unit,), dtype=tf.float32),\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # 全连接网络\n",
    "        return tf.matmul(inputs, self.weight) + self.bias\n",
    "\n",
    "x = tf.ones((3,5))\n",
    "my_layer = MyLayer(5, 4)\n",
    "out = my_layer(x)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按上面构建网络层，图层会自动跟踪权重w和b，当然我们也可以直接用add_weight的方法构建权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.05799209  0.03156646  0.08374223 -0.11691136]\n",
      " [-0.05799209  0.03156646  0.08374223 -0.11691136]\n",
      " [-0.05799209  0.03156646  0.08374223 -0.11691136]], shape=(3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class MyLayer(layers.Layer):\n",
    "    def __init__(self, input_dim=32, unit=32):\n",
    "        super(MyLayer, self).__init__()\n",
    "        # 使用add_weight添加网络变量，使其可追踪\n",
    "        self.weight = self.add_weight(\n",
    "            shape=(input_dim, unit),\n",
    "            initializer=keras.initializers.RandomNormal(),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.bias = self.add_weight(\n",
    "            shape=(unit, ),\n",
    "            initializer=keras.initializers.Zeros(),\n",
    "            trainable=True\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.weight) + self.bias\n",
    "    \n",
    "x = tf.ones((3,5))\n",
    "my_layer = MyLayer(5, 4)\n",
    "out = my_layer(x)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以设置不可训练的权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 3. 3.]\n",
      "[6. 6. 6.]\n",
      "weight: [<tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=array([6., 6., 6.], dtype=float32)>]\n",
      "non-trainable weight: [<tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=array([6., 6., 6.], dtype=float32)>]\n",
      "trainable weight: []\n"
     ]
    }
   ],
   "source": [
    "class AddLayer(layers.Layer):\n",
    "    def __init__(self, input_dim=32):\n",
    "        super(AddLayer, self).__init__()\n",
    "        # 只存储，不训练的变量\n",
    "        self.sum = self.add_weight(shape=(input_dim,),\n",
    "                                initializer=keras.initializers.Zeros(),\n",
    "                                trainable=False)\n",
    "       \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        self.sum.assign_add(tf.reduce_sum(inputs, axis=0))\n",
    "        return self.sum\n",
    "        \n",
    "x = tf.ones((3,3))\n",
    "my_layer = AddLayer(3)\n",
    "out = my_layer(x)\n",
    "print(out.numpy())\n",
    "out = my_layer(x)\n",
    "print(out.numpy())\n",
    "print('weight:', my_layer.weights)\n",
    "print('non-trainable weight:', my_layer.non_trainable_weights)\n",
    "print('trainable weight:', my_layer.trainable_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当定义网络时不知道网络的维度是可以重写build()函数，用获得的shape构建网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.00701024 -0.06287038 -0.30136997]\n",
      " [ 0.00701024 -0.06287038 -0.30136997]\n",
      " [ 0.00701024 -0.06287038 -0.30136997]], shape=(3, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 0.04437653  0.06346857 -0.00314982]\n",
      " [ 0.04437653  0.06346857 -0.00314982]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class MyLayer(layers.Layer):\n",
    "    def __init__(self, unit=32):\n",
    "        super(MyLayer, self).__init__()\n",
    "        self.unit = unit\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # 在build时获取input_shape\n",
    "        self.weight = self.add_weight(shape=(input_shape[-1], self.unit),\n",
    "                                     initializer=keras.initializers.RandomNormal(),\n",
    "                                     trainable=True)\n",
    "        self.bias = self.add_weight(shape=(self.unit,),\n",
    "                                   initializer=keras.initializers.Zeros(),\n",
    "                                   trainable=True)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.weight) + self.bias\n",
    "        \n",
    "\n",
    "my_layer = MyLayer(3)\n",
    "x = tf.ones((3,5))\n",
    "out = my_layer(x)\n",
    "print(out)\n",
    "\n",
    "my_layer = MyLayer(3)\n",
    "x = tf.ones((2,2))\n",
    "out = my_layer(x)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 使用子层递归构建网络层\n",
    "可以在自定义网络层中调用其他自定义网络层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable weights: 0\n",
      "trainable weights: 6\n"
     ]
    }
   ],
   "source": [
    "class MyBlock(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MyBlock, self).__init__()\n",
    "        # 其他自定义网络层\n",
    "        self.layer1 = MyLayer(32)\n",
    "        self.layer2 = MyLayer(16)\n",
    "        self.layer3 = MyLayer(2)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        h1 = self.layer1(inputs)\n",
    "        h1 = tf.nn.relu(h1)\n",
    "        h2 = self.layer2(h1)\n",
    "        h2 = tf.nn.relu(h2)\n",
    "        return self.layer3(h2)\n",
    "    \n",
    "my_block = MyBlock()\n",
    "print('trainable weights:', len(my_block.trainable_weights))\n",
    "y = my_block(tf.ones(shape=(3, 64)))\n",
    "# 构建网络在build()里面，所以执行了才有网络\n",
    "print('trainable weights:', len(my_block.trainable_weights)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以通过构建网络层的方法来收集loss，并可以递归调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "class LossLayer(layers.Layer):\n",
    "    def __init__(self, rate=1e-2):\n",
    "        super(LossLayer, self).__init__()\n",
    "        self.rate = rate\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # 添加loss\n",
    "        self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
    "        return inputs\n",
    "\n",
    "class OutLayer(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(OutLayer, self).__init__()\n",
    "        self.loss_fun=LossLayer(1e-2)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # 就一个loss层\n",
    "        return self.loss_fun(inputs)\n",
    "\n",
    "my_layer = OutLayer()\n",
    "print(len(my_layer.losses)) # 未执行call\n",
    "y = my_layer(tf.zeros(1,1))\n",
    "print(len(my_layer.losses)) # 执行call之后\n",
    "y = my_layer(tf.zeros(1,1))\n",
    "print(len(my_layer.losses)) # call之前会重置为0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果中间调用了keras网络层，里面的正则化loss也会被加入进来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=0.001910928>]\n",
      "[<tf.Variable 'outer_layer/dense/kernel:0' shape=(1, 32) dtype=float32, numpy=\n",
      "array([[-0.07910636,  0.10402972,  0.27718008, -0.33846265, -0.00764611,\n",
      "         0.10265589,  0.33764917, -0.17633584,  0.27937973,  0.11188376,\n",
      "         0.3739105 ,  0.2025696 ,  0.24472874, -0.03522751,  0.27047437,\n",
      "        -0.09395269, -0.39600804, -0.39767784, -0.0683735 , -0.0143874 ,\n",
      "         0.11743826, -0.21127768,  0.03196621, -0.1056419 , -0.3340903 ,\n",
      "         0.38742024, -0.10020471, -0.3868774 ,  0.1689626 , -0.11181715,\n",
      "        -0.36308247,  0.384009  ]], dtype=float32)>, <tf.Variable 'outer_layer/dense/bias:0' shape=(32,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "class OuterLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(OuterLayer, self).__init__()\n",
    "        # 子层中正则化loss也会添加到总的loss中\n",
    "        self.dense = layers.Dense(32, kernel_regularizer=tf.keras.regularizers.l2(1e-3))\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.dense(inputs)\n",
    "\n",
    "my_layer = OuterLayer()\n",
    "y = my_layer(tf.zeros((1,1)))\n",
    "print(my_layer.losses) \n",
    "print(my_layer.weights) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 其他网络层配置\n",
    "### 3.1 使自己的网络层可以序列化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'linear_4', 'trainable': True, 'dtype': 'float32', 'units': 64}\n"
     ]
    }
   ],
   "source": [
    "class Linear(layers.Layer):\n",
    "    def __init__(self, units=32, **kwargs):\n",
    "        super(Linear, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=keras.initializers.RandomNormal(),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units, ),\n",
    "            initializer=keras.initializers.RandomNormal(),\n",
    "            trainable=True\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "    \n",
    "    def get_config(self):\n",
    "        # 获取网络配置实现序列化\n",
    "        config = super(Linear, self).get_config()\n",
    "        config.update({'units': self.units})\n",
    "        return config\n",
    "\n",
    "linear_layer = Linear(units=64)\n",
    "config = linear_layer.get_config()\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(64, 64), dtype=float32, numpy=\n",
       "array([[-0.03712314, -0.02370307,  0.05135906, ...,  0.14805348,\n",
       "         0.00862139,  0.18013929],\n",
       "       [-0.03712314, -0.02370307,  0.05135906, ...,  0.14805348,\n",
       "         0.00862139,  0.18013929],\n",
       "       [-0.03712314, -0.02370307,  0.05135906, ...,  0.14805348,\n",
       "         0.00862139,  0.18013929],\n",
       "       ...,\n",
       "       [-0.03712314, -0.02370307,  0.05135906, ...,  0.14805348,\n",
       "         0.00862139,  0.18013929],\n",
       "       [-0.03712314, -0.02370307,  0.05135906, ...,  0.14805348,\n",
       "         0.00862139,  0.18013929],\n",
       "       [-0.03712314, -0.02370307,  0.05135906, ...,  0.14805348,\n",
       "         0.00862139,  0.18013929]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = linear_layer(tf.ones(shape=(64,3)))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Linear at 0x21bf74153c8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从配置中构建网络，（已知网络结构，不知超参的情况）\n",
    "new_layer = Linear.from_config(config)\n",
    "new_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果在反序列化中(从配置中构建网络)需要更大的灵活性，可以重写from_config方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_config(cls, config):\n",
    "    return cls(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 配置训练时特有参数\n",
    "有一些网络层， 如BatchNormalization层和Dropout层，在训练和推理中具有不同的行为，对于此类层，则需要在方法中使用train等参数进行控制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDropout(layers.Layer):\n",
    "    def __init__(self, rate, **kwargs):\n",
    "        super(MyDropout, self).__init__(**kwargs)\n",
    "        self.rate = rate\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        return tf.cond(training, \n",
    "                    lambda: tf.nn.dropout(inputs, rate=self.rate),\n",
    "                    lambda: inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 构建自己的模型\n",
    "通常，我们使用Layer类来定义内部计算块，并使用Model类来定义外部模型 - 即要训练的对象。\n",
    "\n",
    "Model类与Layer的区别：\n",
    "- 它对外开放内置的训练，评估和预测函数（model.fit(),model.evaluate(),model.predict()）。 \n",
    "- 它通过model.layers属性开放其内部网络层列表。 \n",
    "- 它对外开放保存和序列化API。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 自定义模型\n",
    "下面通过构建一个变分自编码器（VAE），来介绍如何构建自己的网络， 并使用内置的函数进行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 采样网络\n",
    "from itertools import zip_longest\n",
    "class Sampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5*z_log_var) * epsilon\n",
    "    \n",
    "# 编码器\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, latent_dim=32, intermediate_dim=64, name='encoder', **kwargs):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation='relu')\n",
    "        self.dense_mean = layers.Dense(latent_dim)\n",
    "        self.dense_log_var = layers.Dense(latent_dim)\n",
    "        self.sampling = Sampling()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        h1 = self.dense_proj(inputs)\n",
    "        # 获取z_mean或z_log_var\n",
    "        z_mean = self.dense_mean(h1)\n",
    "        z_log_var = self.dense_log_var(h1)\n",
    "        # 进行采样\n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        return z_mean, z_log_var, z\n",
    "    \n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, original_dim, intermediate_dim=64, name='decoder', **kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation='relu')\n",
    "        self.dense_output = layers.Dense(original_dim, activation='sigmoid')\n",
    "    def call(self, inputs):\n",
    "        # 两层全连接网络\n",
    "        h1 = self.dense_proj(inputs)\n",
    "        return self.dense_output(h1)\n",
    "\n",
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, original_dim, latent_dim=32, intermediate_dim=64, name='encoder', **kwargs):\n",
    "        super(VAE, self).__init__(name=name, **kwargs)\n",
    "        self.original_dim = original_dim\n",
    "        self.encoder = Encoder(latent_dim=latent_dim, intermediate_dim=intermediate_dim)\n",
    "        self.decoder = Decoder(original_dim=original_dim, intermediate_dim=intermediate_dim)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # 编码\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        # 解码\n",
    "        reconstructed = self.decoder(z)\n",
    "        # 获取损失函数\n",
    "        kl_loss = -0.5 * tf.reduce_sum(\n",
    "            z_log_var-tf.square(z_mean)-tf.exp(z_log_var)+1\n",
    "        )\n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "938/938 [==============================] - 3s 2ms/step - loss: 0.7666\n",
      "Epoch 2/3\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0688\n",
      "Epoch 3/3\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0678\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21bf7701f88>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
    "vae = VAE(784, latent_dim=32, intermediate_dim=64)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
    "vae.fit(x_train, x_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "step 0: mean loss = tf.Tensor(225.1166, shape=(), dtype=float32)\n",
      "step 100: mean loss = tf.Tensor(6.628161, shape=(), dtype=float32)\n",
      "step 200: mean loss = tf.Tensor(3.3782306, shape=(), dtype=float32)\n",
      "step 300: mean loss = tf.Tensor(2.2841198, shape=(), dtype=float32)\n",
      "step 400: mean loss = tf.Tensor(1.7342669, shape=(), dtype=float32)\n",
      "step 500: mean loss = tf.Tensor(1.4038577, shape=(), dtype=float32)\n",
      "step 600: mean loss = tf.Tensor(1.1826853, shape=(), dtype=float32)\n",
      "step 700: mean loss = tf.Tensor(1.0242989, shape=(), dtype=float32)\n",
      "step 800: mean loss = tf.Tensor(0.9054826, shape=(), dtype=float32)\n",
      "step 900: mean loss = tf.Tensor(0.8128643, shape=(), dtype=float32)\n",
      "Start of epoch 1\n",
      "step 0: mean loss = tf.Tensor(0.782809, shape=(), dtype=float32)\n",
      "step 100: mean loss = tf.Tensor(0.7141648, shape=(), dtype=float32)\n",
      "step 200: mean loss = tf.Tensor(0.65757877, shape=(), dtype=float32)\n",
      "step 300: mean loss = tf.Tensor(0.61011475, shape=(), dtype=float32)\n",
      "step 400: mean loss = tf.Tensor(0.56973755, shape=(), dtype=float32)\n",
      "step 500: mean loss = tf.Tensor(0.5349032, shape=(), dtype=float32)\n",
      "step 600: mean loss = tf.Tensor(0.50462556, shape=(), dtype=float32)\n",
      "step 700: mean loss = tf.Tensor(0.47798935, shape=(), dtype=float32)\n",
      "step 800: mean loss = tf.Tensor(0.45442948, shape=(), dtype=float32)\n",
      "step 900: mean loss = tf.Tensor(0.4333643, shape=(), dtype=float32)\n",
      "Start of epoch 2\n",
      "step 0: mean loss = tf.Tensor(0.4259578, shape=(), dtype=float32)\n",
      "step 100: mean loss = tf.Tensor(0.407837, shape=(), dtype=float32)\n",
      "step 200: mean loss = tf.Tensor(0.39149866, shape=(), dtype=float32)\n",
      "step 300: mean loss = tf.Tensor(0.3766331, shape=(), dtype=float32)\n",
      "step 400: mean loss = tf.Tensor(0.3631125, shape=(), dtype=float32)\n",
      "step 500: mean loss = tf.Tensor(0.35066158, shape=(), dtype=float32)\n",
      "step 600: mean loss = tf.Tensor(0.3392458, shape=(), dtype=float32)\n",
      "step 700: mean loss = tf.Tensor(0.32869235, shape=(), dtype=float32)\n",
      "step 800: mean loss = tf.Tensor(0.3189432, shape=(), dtype=float32)\n",
      "step 900: mean loss = tf.Tensor(0.3098613, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "original_dim= 784\n",
    "vae = VAE(original_dim, 64, 32)\n",
    "\n",
    "# 优化器\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "# 损失函数\n",
    "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "# 评价指标\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(3):\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "    for step, x_batch_train in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # 前向传播\n",
    "            reconstructed = vae(x_batch_train)\n",
    "            # 计算loss\n",
    "            loss = mse_loss_fn(x_batch_train, reconstructed)\n",
    "            loss += sum(vae.losses) # Add KLD regularization loss\n",
    "        \n",
    "        # 计算梯度\n",
    "        grads = tape.gradient(loss, vae.trainable_variables)\n",
    "        # 反向传播\n",
    "        optimizer.apply_gradients(zip(grads, vae.trainable_variables))\n",
    "        # 统计指标\n",
    "        loss_metric(loss)\n",
    "        # 输出\n",
    "        if step % 100 == 0:\n",
    "            print('step %s: mean loss = %s' % (step, loss_metric.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('work')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "530219f8a9f45de8614553891b486f072fd396fb4547030fbb192564875089f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
